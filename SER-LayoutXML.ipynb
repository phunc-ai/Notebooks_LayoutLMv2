{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SER-LayoutXML.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOhv9sL6nW5hZdz8pj8SHe6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fXxzU9gAw3GY","executionInfo":{"status":"ok","timestamp":1649763866640,"user_tz":-420,"elapsed":15,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"outputs":[],"source":["# !git clone https://github.com/microsoft/unilm.git"]},{"cell_type":"code","source":["%cd unilm\n","%cd layoutlmft"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJyt4si047DT","executionInfo":{"status":"ok","timestamp":1649763866641,"user_tz":-420,"elapsed":15,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"00a6391d-72a5-4b3e-c18b-166825b2397f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/unilm\n","/content/unilm/layoutlmft\n"]}]},{"cell_type":"code","source":["# !pip install -r requirements.txt\n","# !pip install -e .\n","# !pip install ."],"metadata":{"id":"nfJFGucBxLZw","executionInfo":{"status":"ok","timestamp":1649763866642,"user_tz":-420,"elapsed":13,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","os.chdir('/content/unilm/layoutlmft')"],"metadata":{"id":"XZdb-ots6BRu","executionInfo":{"status":"ok","timestamp":1649763866643,"user_tz":-420,"elapsed":13,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# !python -m torch.distributed.launch --nproc_per_node=1 examples/run_xfun_ser.py \\\n","#         --model_name_or_path microsoft/layoutxlm-base \\\n","#         --output_dir /tmp/test-ner \\\n","#         --do_train \\\n","#         --do_eval \\\n","#         --lang ja \\\n","#         --max_steps 3000 \\\n","#         --warmup_ratio 0.1 \\\n","#         --per_gpu_train_batch_size 2 \\\n","#         --fp16"],"metadata":{"id":"ipscuy-3xzOv","executionInfo":{"status":"ok","timestamp":1649763866644,"user_tz":-420,"elapsed":14,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!python -m torch.distributed.launch --nproc_per_node=1 examples/run_xfun_re.py \\\n","        --model_name_or_path microsoft/layoutxlm-base \\\n","        --output_dir /tmp/test-re \\\n","        --do_train \\\n","        --do_eval \\\n","        --lang zh \\\n","        --max_steps 2500 \\\n","        --per_device_train_batch_size 2 \\\n","        --warmup_ratio 0.1 \\\n","        --fp16"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwdZ11EYA4Zw","executionInfo":{"status":"ok","timestamp":1649765869465,"user_tz":-420,"elapsed":2002834,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"67366122-9fd3-49f1-9a5d-533049a6f1db"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 615/615 [00:00<00:00, 672kB/s]\n","Downloading: 100% 5.07M/5.07M [00:00<00:00, 7.25MB/s]\n","Downloading: 100% 9.10M/9.10M [00:00<00:00, 10.7MB/s]\n","04/12/2022 11:44:39 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n","04/12/2022 11:44:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-re, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=2500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.1, warmup_steps=0, logging_dir=runs/Apr12_11-44-36_222c7fb31692, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-re, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n","[INFO|configuration_utils.py:491] 2022-04-12 11:44:39,849 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n","[INFO|configuration_utils.py:527] 2022-04-12 11:44:39,849 >> Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.5.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:44:41,652 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:44:41,652 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:44:41,652 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:44:41,652 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:44:41,652 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n","Downloading and preparing dataset xfun/xfun.zh (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/xfun/xfun.zh/0.0.0/affa7f771c23899f4ea7b3b522db75470abe55a08e8cf96de60597348837b9ed...\n","Downloading: 100% 4.67M/4.67M [00:00<00:00, 63.1MB/s]\n","Downloading: 100% 206M/206M [00:06<00:00, 33.8MB/s]\n","Downloading: 100% 1.71M/1.71M [00:00<00:00, 55.7MB/s]\n","Downloading: 100% 69.2M/69.2M [00:12<00:00, 5.49MB/s]\n","Dataset xfun downloaded and prepared to /root/.cache/huggingface/datasets/xfun/xfun.zh/0.0.0/affa7f771c23899f4ea7b3b522db75470abe55a08e8cf96de60597348837b9ed. Subsequent calls will reuse this data.\n","[INFO|file_utils.py:1426] 2022-04-12 11:46:20,894 >> https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpik21jwrs\n","Downloading: 100% 1.02k/1.02k [00:00<00:00, 899kB/s]\n","[INFO|file_utils.py:1430] 2022-04-12 11:46:21,258 >> storing https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n","[INFO|file_utils.py:1433] 2022-04-12 11:46:21,258 >> creating metadata file for /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n","[INFO|configuration_utils.py:491] 2022-04-12 11:46:21,259 >> loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n","[INFO|configuration_utils.py:527] 2022-04-12 11:46:21,259 >> Model config LayoutLMv2Config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"configuration_files\": [\n","    \"config.json\",\n","    \"config.4.13.0.json\"\n","  ],\n","  \"convert_sync_batchnorm\": true,\n","  \"coordinate_size\": 128,\n","  \"eos_token_id\": 2,\n","  \"fast_qkv\": false,\n","  \"finetuning_task\": \"ner\",\n","  \"gradient_checkpointing\": false,\n","  \"has_relative_attention_bias\": false,\n","  \"has_spatial_attention_bias\": false,\n","  \"has_visual_segment_embedding\": true,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"image_feature_pool_shape\": [\n","    7,\n","    7,\n","    256\n","  ],\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_2d_position_embeddings\": 1024,\n","  \"max_position_embeddings\": 514,\n","  \"max_rel_2d_pos\": 256,\n","  \"max_rel_pos\": 128,\n","  \"model_type\": \"layoutlmv2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"rel_2d_pos_bins\": 64,\n","  \"rel_pos_bins\": 32,\n","  \"shape_size\": 128,\n","  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n","  \"transformers_version\": \"4.5.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|configuration_utils.py:491] 2022-04-12 11:46:21,624 >> loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n","[INFO|configuration_utils.py:527] 2022-04-12 11:46:21,624 >> Model config LayoutLMv2Config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"configuration_files\": [\n","    \"config.json\",\n","    \"config.4.13.0.json\"\n","  ],\n","  \"convert_sync_batchnorm\": true,\n","  \"coordinate_size\": 128,\n","  \"eos_token_id\": 2,\n","  \"fast_qkv\": false,\n","  \"gradient_checkpointing\": false,\n","  \"has_relative_attention_bias\": false,\n","  \"has_spatial_attention_bias\": false,\n","  \"has_visual_segment_embedding\": true,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"image_feature_pool_shape\": [\n","    7,\n","    7,\n","    256\n","  ],\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_2d_position_embeddings\": 1024,\n","  \"max_position_embeddings\": 514,\n","  \"max_rel_2d_pos\": 256,\n","  \"max_rel_pos\": 128,\n","  \"model_type\": \"layoutlmv2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"rel_2d_pos_bins\": 64,\n","  \"rel_pos_bins\": 32,\n","  \"shape_size\": 128,\n","  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n","  \"transformers_version\": \"4.5.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|file_utils.py:1426] 2022-04-12 11:46:21,990 >> https://huggingface.co/microsoft/layoutxlm-base/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpiq6o59rp\n","Downloading: 100% 5.07M/5.07M [00:00<00:00, 63.9MB/s]\n","[INFO|file_utils.py:1430] 2022-04-12 11:46:22,103 >> storing https://huggingface.co/microsoft/layoutxlm-base/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/huggingface/transformers/db844fb5c32918721ed233d2411496a5dad53b43140ce01851ee7ee5d1403528.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","[INFO|file_utils.py:1433] 2022-04-12 11:46:22,103 >> creating metadata file for /root/.cache/huggingface/transformers/db844fb5c32918721ed233d2411496a5dad53b43140ce01851ee7ee5d1403528.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","[INFO|file_utils.py:1426] 2022-04-12 11:46:22,461 >> https://huggingface.co/microsoft/layoutxlm-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1akwg68r\n","Downloading: 100% 9.10M/9.10M [00:00<00:00, 11.8MB/s]\n","[INFO|file_utils.py:1430] 2022-04-12 11:46:23,934 >> storing https://huggingface.co/microsoft/layoutxlm-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/c9202fe65b5d10a23dc6251d2a9b14829f1381416c511d376cf456ed8b5b5c01.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","[INFO|file_utils.py:1433] 2022-04-12 11:46:23,934 >> creating metadata file for /root/.cache/huggingface/transformers/c9202fe65b5d10a23dc6251d2a9b14829f1381416c511d376cf456ed8b5b5c01.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:46:25,025 >> loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/db844fb5c32918721ed233d2411496a5dad53b43140ce01851ee7ee5d1403528.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:46:25,025 >> loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/c9202fe65b5d10a23dc6251d2a9b14829f1381416c511d376cf456ed8b5b5c01.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:46:25,025 >> loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:46:25,025 >> loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1707] 2022-04-12 11:46:25,025 >> loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|file_utils.py:1426] 2022-04-12 11:46:25,820 >> https://huggingface.co/microsoft/layoutxlm-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvo9z0k2l\n","Downloading: 100% 1.48G/1.48G [00:20<00:00, 71.4MB/s]\n","[INFO|file_utils.py:1430] 2022-04-12 11:46:46,523 >> storing https://huggingface.co/microsoft/layoutxlm-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8680422ada73a219d10ded26623c015f44a909e815304488fd43ed77efe03e27.89dd075ca1cb2d01705599c66b2867ecedda5e15879080b08735d2dbdf3631b7\n","[INFO|file_utils.py:1433] 2022-04-12 11:46:46,524 >> creating metadata file for /root/.cache/huggingface/transformers/8680422ada73a219d10ded26623c015f44a909e815304488fd43ed77efe03e27.89dd075ca1cb2d01705599c66b2867ecedda5e15879080b08735d2dbdf3631b7\n","[INFO|modeling_utils.py:1052] 2022-04-12 11:46:46,524 >> loading weights file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8680422ada73a219d10ded26623c015f44a909e815304488fd43ed77efe03e27.89dd075ca1cb2d01705599c66b2867ecedda5e15879080b08735d2dbdf3631b7\n","[INFO|modeling_utils.py:1168] 2022-04-12 11:46:57,583 >> All model checkpoint weights were used when initializing LayoutLMv2ForRelationExtraction.\n","\n","[WARNING|modeling_utils.py:1171] 2022-04-12 11:46:57,583 >> Some weights of LayoutLMv2ForRelationExtraction were not initialized from the model checkpoint at microsoft/layoutxlm-base and are newly initialized: ['extractor.entity_emb.weight', 'extractor.ffnn_head.0.weight', 'extractor.ffnn_head.0.bias', 'extractor.ffnn_head.3.weight', 'extractor.ffnn_head.3.bias', 'extractor.ffnn_tail.0.weight', 'extractor.ffnn_tail.0.bias', 'extractor.ffnn_tail.3.weight', 'extractor.ffnn_tail.3.bias', 'extractor.rel_classifier.bilinear.weight', 'extractor.rel_classifier.linear.weight', 'extractor.rel_classifier.linear.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer.py:377] 2022-04-12 11:46:58,986 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:491] 2022-04-12 11:46:58,987 >> The following columns in the training set  don't have a corresponding argument in `LayoutLMv2ForRelationExtraction.forward` and have been ignored: id.\n","[INFO|trainer.py:491] 2022-04-12 11:46:58,987 >> The following columns in the evaluation set  don't have a corresponding argument in `LayoutLMv2ForRelationExtraction.forward` and have been ignored: id.\n","[INFO|trainer.py:402] 2022-04-12 11:46:58,988 >> Using amp fp16 backend\n","[INFO|trainer.py:1013] 2022-04-12 11:46:59,294 >> ***** Running training *****\n","[INFO|trainer.py:1014] 2022-04-12 11:46:59,294 >>   Num examples = 187\n","[INFO|trainer.py:1015] 2022-04-12 11:46:59,294 >>   Num Epochs = 27\n","[INFO|trainer.py:1016] 2022-04-12 11:46:59,294 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1017] 2022-04-12 11:46:59,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1018] 2022-04-12 11:46:59,294 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1019] 2022-04-12 11:46:59,294 >>   Total optimization steps = 2500\n","  0% 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","{'loss': 0.4221, 'learning_rate': 4.4444444444444447e-05, 'epoch': 5.32}\n"," 20% 500/2500 [05:50<23:57,  1.39it/s][INFO|trainer.py:1648] 2022-04-12 11:52:49,677 >> Saving model checkpoint to /tmp/test-re/checkpoint-500\n","[INFO|configuration_utils.py:329] 2022-04-12 11:52:49,678 >> Configuration saved in /tmp/test-re/checkpoint-500/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 11:52:56,553 >> Model weights saved in /tmp/test-re/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 11:52:56,554 >> tokenizer config file saved in /tmp/test-re/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 11:52:56,554 >> Special tokens file saved in /tmp/test-re/checkpoint-500/special_tokens_map.json\n","{'loss': 0.1707, 'learning_rate': 3.3333333333333335e-05, 'epoch': 10.64}\n"," 40% 1000/2500 [11:59<16:24,  1.52it/s][INFO|trainer.py:1648] 2022-04-12 11:58:58,557 >> Saving model checkpoint to /tmp/test-re/checkpoint-1000\n","[INFO|configuration_utils.py:329] 2022-04-12 11:58:58,558 >> Configuration saved in /tmp/test-re/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 11:59:03,335 >> Model weights saved in /tmp/test-re/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 11:59:03,336 >> tokenizer config file saved in /tmp/test-re/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 11:59:03,337 >> Special tokens file saved in /tmp/test-re/checkpoint-1000/special_tokens_map.json\n","{'loss': 0.0761, 'learning_rate': 2.2222222222222223e-05, 'epoch': 15.96}\n"," 60% 1500/2500 [18:09<12:06,  1.38it/s][INFO|trainer.py:1648] 2022-04-12 12:05:09,078 >> Saving model checkpoint to /tmp/test-re/checkpoint-1500\n","[INFO|configuration_utils.py:329] 2022-04-12 12:05:09,079 >> Configuration saved in /tmp/test-re/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 12:05:13,669 >> Model weights saved in /tmp/test-re/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 12:05:13,670 >> tokenizer config file saved in /tmp/test-re/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 12:05:13,670 >> Special tokens file saved in /tmp/test-re/checkpoint-1500/special_tokens_map.json\n","{'loss': 0.0356, 'learning_rate': 1.1111111111111112e-05, 'epoch': 21.28}\n"," 80% 2000/2500 [24:15<05:38,  1.48it/s][INFO|trainer.py:1648] 2022-04-12 12:11:15,156 >> Saving model checkpoint to /tmp/test-re/checkpoint-2000\n","[INFO|configuration_utils.py:329] 2022-04-12 12:11:15,157 >> Configuration saved in /tmp/test-re/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 12:11:20,121 >> Model weights saved in /tmp/test-re/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 12:11:20,122 >> tokenizer config file saved in /tmp/test-re/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 12:11:20,122 >> Special tokens file saved in /tmp/test-re/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.0182, 'learning_rate': 0.0, 'epoch': 26.6}\n","100% 2500/2500 [30:19<00:00,  1.49it/s][INFO|trainer.py:1648] 2022-04-12 12:17:18,476 >> Saving model checkpoint to /tmp/test-re/checkpoint-2500\n","[INFO|configuration_utils.py:329] 2022-04-12 12:17:18,477 >> Configuration saved in /tmp/test-re/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 12:17:23,589 >> Model weights saved in /tmp/test-re/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 12:17:23,590 >> tokenizer config file saved in /tmp/test-re/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 12:17:23,591 >> Special tokens file saved in /tmp/test-re/checkpoint-2500/special_tokens_map.json\n","[INFO|trainer.py:1196] 2022-04-12 12:17:34,869 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1835.5751, 'train_samples_per_second': 1.362, 'epoch': 26.6}\n","100% 2500/2500 [30:35<00:00,  1.36it/s]\n","[INFO|trainer.py:1648] 2022-04-12 12:17:35,152 >> Saving model checkpoint to /tmp/test-re\n","[INFO|configuration_utils.py:329] 2022-04-12 12:17:35,153 >> Configuration saved in /tmp/test-re/config.json\n","[INFO|modeling_utils.py:831] 2022-04-12 12:17:40,094 >> Model weights saved in /tmp/test-re/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1901] 2022-04-12 12:17:40,095 >> tokenizer config file saved in /tmp/test-re/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1907] 2022-04-12 12:17:40,096 >> Special tokens file saved in /tmp/test-re/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:722] 2022-04-12 12:17:40,439 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   epoch                      =       26.6\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   init_mem_cpu_alloc_delta   =        0MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   init_mem_gpu_alloc_delta   =     1428MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   init_mem_gpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_mem_cpu_alloc_delta  =     -651MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_mem_cpu_peaked_delta =      775MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_mem_gpu_alloc_delta  =     5711MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_mem_gpu_peaked_delta =     4820MB\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_runtime              = 0:30:35.57\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_samples              =        187\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:40,440 >>   train_samples_per_second   =      1.362\n","04/12/2022 12:17:40 - INFO - __main__ -   *** Evaluate ***\n","100% 9/9 [00:04<00:00,  2.05it/s]\n","[INFO|trainer_pt_utils.py:722] 2022-04-12 12:17:45,337 >> ***** eval metrics *****\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   epoch                   =       26.6\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_f1                 =     0.6726\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_loss               =     0.0547\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_precision          =     0.5796\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_recall             =     0.8012\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_runtime            = 0:00:04.89\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_samples            =         65\n","[INFO|trainer_pt_utils.py:727] 2022-04-12 12:17:45,337 >>   eval_samples_per_second =     13.282\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"trPPPr0RHsBM","executionInfo":{"status":"ok","timestamp":1649765869465,"user_tz":-420,"elapsed":6,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":6,"outputs":[]}]}